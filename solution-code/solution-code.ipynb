{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a soup object from the home page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the requests library to get the html from the home page\n",
    "res = requests.get('http://chubbygrub.com')\n",
    "\n",
    "# Create a soup object from the html\n",
    "soup = bs(res.content, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Scrape the home page soup for every restaurant\n",
    "\n",
    "Note: Your best bet is to create a list of dictionaries, one for each restaurant. Each dictionary contains the restaurant's name and slug. The result of your scrape should look something like this:\n",
    "\n",
    "```python\n",
    "restaurants = [\n",
    "    {'name': 'A&W Restaurants', 'slug': 'aw-restaurants'}, \n",
    "    {'name': \"Applebee's\", 'slug': 'applebees'},\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'A&W Restaurants', 'slug': 'aw-restaurants'},\n",
       " {'name': \"Applebee's\", 'slug': 'applebees'},\n",
       " {'name': \"Arby's\", 'slug': 'arbys'},\n",
       " {'name': 'Atlanta Bread Company', 'slug': 'atlanta-bread-company'},\n",
       " {'name': \"Bojangle's Famous Chicken 'n Biscuits\",\n",
       "  'slug': 'bojangles-famous-chicken-n-biscuits'},\n",
       " {'name': 'Buffalo Wild Wings', 'slug': 'buffalo-wild-wings'},\n",
       " {'name': 'Burger King', 'slug': 'burger-king'},\n",
       " {'name': \"Captain D's\", 'slug': 'captain-ds'},\n",
       " {'name': \"Carl's Jr.\", 'slug': 'carls-jr'},\n",
       " {'name': \"Charley's Grilled Subs\", 'slug': 'charleys-grilled-subs'},\n",
       " {'name': 'Chick-fil-A', 'slug': 'chick-fil-a'},\n",
       " {'name': \"Chili's\", 'slug': 'chilis'},\n",
       " {'name': 'Chipotle Mexican Grill', 'slug': 'chipotle-mexican-grill'},\n",
       " {'name': \"Church's\", 'slug': 'churchs'},\n",
       " {'name': 'Corner Bakery Cafe', 'slug': 'corner-bakery-cafe'},\n",
       " {'name': 'Dairy Queen', 'slug': 'dairy-queen'},\n",
       " {'name': \"Denny's\", 'slug': 'dennys'},\n",
       " {'name': 'El Pollo Loco', 'slug': 'el-pollo-loco'},\n",
       " {'name': 'FATZ', 'slug': 'fatz'},\n",
       " {'name': \"Fazoli's\", 'slug': 'fazolis'},\n",
       " {'name': 'Five Guys Burgers and Fries',\n",
       "  'slug': 'five-guys-burgers-and-fries'},\n",
       " {'name': 'Golden Chick', 'slug': 'golden-chick'},\n",
       " {'name': \"Hardee's\", 'slug': 'hardees'},\n",
       " {'name': 'IHOP', 'slug': 'ihop'},\n",
       " {'name': 'In-N-Out Burger', 'slug': 'in-n-out-burger'},\n",
       " {'name': 'Jack in the Box', 'slug': 'jack-in-the-box'},\n",
       " {'name': 'Jimmy Johns', 'slug': 'jimmy-johns'},\n",
       " {'name': \"Joe's Crab Shack\", 'slug': 'joes-crab-shack'},\n",
       " {'name': 'KFC', 'slug': 'kfc'},\n",
       " {'name': \"McDonald's\", 'slug': 'mcdonalds'},\n",
       " {'name': \"O'Charley's\", 'slug': 'ocharleys'},\n",
       " {'name': 'Olive Garden', 'slug': 'olive-garden'},\n",
       " {'name': 'Outback Steakhouse', 'slug': 'outback-steakhouse'},\n",
       " {'name': 'Panda Express', 'slug': 'panda-express'},\n",
       " {'name': 'Panera Bread', 'slug': 'panera-bread'},\n",
       " {'name': \"Popeye's\", 'slug': 'popeyes'},\n",
       " {'name': 'Quiznos', 'slug': 'quiznos'},\n",
       " {'name': 'Red Robin Gourmet Burgers', 'slug': 'red-robin-gourmet-burgers'},\n",
       " {'name': \"Romano's Macaroni Grill\", 'slug': 'romanos-macaroni-grill'},\n",
       " {'name': 'Ruby Tuesday', 'slug': 'ruby-tuesday'},\n",
       " {'name': 'Subway', 'slug': 'subway'},\n",
       " {'name': 'Taco Bell', 'slug': 'taco-bell'},\n",
       " {'name': 'Taco Bueno', 'slug': 'taco-bueno'},\n",
       " {'name': \"Wendy's\", 'slug': 'wendys'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Isolate the div that has all the restaurant links\n",
    "restaurants_section = soup.find('div', {'class': 'restaurant-buttons'})\n",
    "\n",
    "# Create an empty list\n",
    "restaurants = []\n",
    "\n",
    "# Loop through each link in the restaurants div\n",
    "for restaurant_link in restaurants_section.find_all('a', {'class': 'btn btn-lg btn-primary'}):\n",
    "    # Start with an empty dictionary\n",
    "    restaurant = {}\n",
    "    \n",
    "    # Add name\n",
    "    restaurant['name'] = restaurant_link.text\n",
    "    \n",
    "    # Add slug from the href\n",
    "    restaurant['slug'] = restaurant_link['href'].split('/')[-1]\n",
    "    \n",
    "    # Add restaurant to our list of restaurants\n",
    "    restaurants.append(restaurant)\n",
    "restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Using the slug, scrape each restaurant's page and create a single list of food dictionaries.\n",
    "\n",
    "Your list of foods should look something like this:\n",
    "```python\n",
    "foods = [\n",
    "    {\n",
    "        'calories': '0',\n",
    "        'carbs': '0',\n",
    "        'category': 'Drinks',\n",
    "        'fat': '0',\n",
    "        'name': 'A&W® Diet Root Beer',\n",
    "        'restaurant': 'A&W Restaurants'\n",
    "    },\n",
    "    {\n",
    "        'calories': '0',\n",
    "        'carbs': '0',\n",
    "        'category': 'Drinks',\n",
    "        'fat': '0',\n",
    "        'name': 'A&W® Diet Root Beer',\n",
    "        'restaurant': 'A&W Restaurants'\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "**Note**: Remove extra white space from each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping A&W Restaurants\n",
      "Scraping Applebee's\n",
      "Scraping Arby's\n",
      "Scraping Atlanta Bread Company\n",
      "Scraping Bojangle's Famous Chicken 'n Biscuits\n",
      "Scraping Buffalo Wild Wings\n",
      "Scraping Burger King\n",
      "Scraping Captain D's\n",
      "Scraping Carl's Jr.\n",
      "Scraping Charley's Grilled Subs\n",
      "Scraping Chick-fil-A\n",
      "Scraping Chili's\n",
      "Scraping Chipotle Mexican Grill\n",
      "Scraping Church's\n",
      "Scraping Corner Bakery Cafe\n",
      "Scraping Dairy Queen\n",
      "Scraping Denny's\n",
      "Scraping El Pollo Loco\n",
      "Scraping FATZ\n",
      "Scraping Fazoli's\n",
      "Scraping Five Guys Burgers and Fries\n",
      "Scraping Golden Chick\n",
      "Scraping Hardee's\n",
      "Scraping IHOP\n",
      "Scraping In-N-Out Burger\n",
      "Scraping Jack in the Box\n",
      "Scraping Jimmy Johns\n",
      "Scraping Joe's Crab Shack\n",
      "Scraping KFC\n",
      "Scraping McDonald's\n",
      "Scraping O'Charley's\n",
      "Scraping Olive Garden\n",
      "Scraping Outback Steakhouse\n",
      "Scraping Panda Express\n",
      "Scraping Panera Bread\n",
      "Scraping Popeye's\n",
      "Scraping Quiznos\n",
      "Scraping Red Robin Gourmet Burgers\n",
      "Scraping Romano's Macaroni Grill\n",
      "Scraping Ruby Tuesday\n",
      "Scraping Subway\n",
      "Scraping Taco Bell\n",
      "Scraping Taco Bueno\n",
      "Scraping Wendy's\n"
     ]
    }
   ],
   "source": [
    "# Start with an empty list\n",
    "foods = []\n",
    "\n",
    "# Loop through each restaurant in the previous step\n",
    "for restaurant in restaurants:\n",
    "    print('Scraping {}'.format(restaurant['name']))\n",
    "    \n",
    "    # Use requests library to get the content from each restaurant page\n",
    "    restaurant_res = requests.get('http://chubbygrub.com/restaurants/{}'.format(restaurant['slug']))\n",
    "    \n",
    "    # Create soup object from restauarant html\n",
    "    restaurant_soup = bs(restaurant_res.content, 'lxml')\n",
    "    \n",
    "    # Isolate the foods table from restaurant page\n",
    "    table = restaurant_soup.find('table', {'id': 'items'})\n",
    "    \n",
    "    # Loop through each row in the tbody of the restaurants table\n",
    "    for row in table.find('tbody').find_all('tr'):\n",
    "        # We'll use almost all the <td /> tags for each row, might as well create a variable\n",
    "        cells = row.find_all('td')\n",
    "        \n",
    "        # Start with an empty food dictionary\n",
    "        food = {}\n",
    "        \n",
    "        # Add the restaurant's name (No need for the slug, that was just for scraping purposes)\n",
    "        food['restaurant'] = restaurant['name']\n",
    "        \n",
    "        # Add food name from firs cell\n",
    "        food['name'] = cells[0].text\n",
    "        \n",
    "        # Add category, note the .strip() for removing white space\n",
    "        food['category'] = cells[1].text.strip()\n",
    "        \n",
    "        # Add calories \n",
    "        food['calories'] = cells[2].text\n",
    "        \n",
    "        # Add fat\n",
    "        food['fat'] = cells[3].text\n",
    "        \n",
    "        # Add carbs\n",
    "        food['carbs'] = cells[4].text\n",
    "        \n",
    "        # Add the food dictionary to our list of foods\n",
    "        foods.append(food)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create a pandas DataFrame from your list of foods\n",
    "\n",
    "**Note**: Your DataFrame should have 4,977 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4977, 6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame from our list of dictionaries\n",
    "df = pd.DataFrame(foods)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Export to csv\n",
    "\n",
    "**Note:** Don't export the index column from your DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "df.to_csv('foods.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
